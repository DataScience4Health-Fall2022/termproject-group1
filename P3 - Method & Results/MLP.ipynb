{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3a293ee5",
      "metadata": {
        "id": "3a293ee5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "pd.options.display.max_seq_items = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f50a2e52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f50a2e52",
        "outputId": "3fabef8d-68bf-4076-c554-ee0bc8d25483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## setting up drive to import the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/drive/My Drive/dataset_OSAS.pickle', 'rb')\n",
        "data = pickle.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJFvPJn33X_l",
        "outputId": "9f0338ef-221d-479a-df22-eab37cccda19"
      },
      "id": "MJFvPJn33X_l",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "961357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  ## helper functions for the logistic sigmoid function\n",
        "  def logistic(x):\n",
        "    return 1.0/(1+np.exp(-x))\n",
        "\n",
        "  def logistic_deriv(x):\n",
        "    return logistic(x) * (1-logistic(x))\n",
        "\n",
        "  ## initializing the learning rate, input and hidden layer dimensionality and the epochs \n",
        "  LR = 1\n",
        "\n",
        "  I_dim = 4\n",
        "  H_dim = 4 \n",
        "\n",
        "  epoch_count = 20\n",
        "\n",
        "  ## starting with random weight initilization\n",
        "  weights_ItoH = np.random.uniform(-1, 1, (I_dim,H_dim))\n",
        "  weights_HtoO = np.random.uniform(-1,1,H_dim)\n",
        "\n",
        "  preActivation_H = np.zeros(H_dim)\n",
        "  postActivation_H = np.zeros(H_dim)\n"
      ],
      "metadata": {
        "id": "ItGjaBcOE9jT"
      },
      "id": "ItGjaBcOE9jT",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  #helping function to check the number of true values\n",
        "\n",
        "  def count_true(list):\n",
        "    count = 0\n",
        "    for i in list:\n",
        "      if i == 1:\n",
        "        count+=1\n",
        "    return count\n",
        "    \n"
      ],
      "metadata": {
        "id": "Tk_2zmRdrYBn"
      },
      "id": "Tk_2zmRdrYBn",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  ## This cell sets up our training and testing data \n",
        "  patient_ids = np.unique(data['patient'])\n",
        "  patient_count = len(patient_ids)\n",
        "\n",
        "  training_count = int(patient_count * .8)\n",
        "  testing_count = int(patient_count * .2)\n",
        "\n",
        "  assert( (training_count + testing_count) == patient_count)\n",
        "\n",
        "  training_patients = patient_ids[:training_count]\n",
        "  testing_patients = patient_ids[training_count:]\n",
        "\n",
        "  training_data_set = np.array([])\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  cnt = 0\n",
        "\n",
        "  #preparing training data \n",
        "  for pid in training_patients:\n",
        "    patient = data[data['patient'] == pid]\n",
        "    \n",
        "    ## the features we are using to train the model\n",
        "    rr = patient['RR(rpm)'].to_numpy()\n",
        "    hr = patient['HR(bpm)'].to_numpy()\n",
        "    spO2 = patient['SpO2(%)'].to_numpy()\n",
        "    pvcs = patient['PVCs(/min)'].to_numpy()\n",
        "\n",
        "    labels = np.array( list( map( lambda x: 1 if x.__contains__('APNEA') else 0, patient['event'].to_list())))\n",
        " \n",
        "    for i in range(len(rr)):\n",
        "        if (np.isnan(rr[i]) or np.isnan(hr[i]) or np.isnan(spO2[i]) or np.isnan(pvcs[i])):\n",
        "            continue\n",
        "        X.append([rr[i], hr[i], spO2[i], pvcs[i]])\n",
        "        y.append(labels[i])\n",
        "        \n",
        "    \n",
        "    training_data = np.asarray(X)\n",
        "    training_count = len(training_data)\n",
        "\n",
        "  ## standardizing and handling the imbalance of our raw data \n",
        "  X = StandardScaler().fit_transform(X)\n",
        "  oversample = SMOTE()\n",
        "  X, y = oversample.fit_resample(X, y)\n",
        "   \n",
        "  #preparing testing data \n",
        "  X_t = []\n",
        "  y_t = []\n",
        "  for pid in testing_patients:\n",
        "    patient = data[data['patient'] == pid]\n",
        "    \n",
        "    rr = patient['RR(rpm)'].to_numpy()\n",
        "    hr = patient['HR(bpm)'].to_numpy()\n",
        "    spO2 = patient['SpO2(%)'].to_numpy()\n",
        "    pvcs = patient['PVCs(/min)'].to_numpy()\n",
        "\n",
        "    labels = np.array( list( map( lambda x: 1 if x.__contains__('APNEA') else 0, patient['event'].to_list())))\n",
        "    # print(len(labels))\n",
        "    # print(patient.head())\n",
        "    # print(np.unique(patient['event']))\n",
        "    for i in range(len(rr)):\n",
        "        if (np.isnan(rr[i]) or np.isnan(hr[i]) or np.isnan(spO2[i]) or np.isnan(pvcs[i])):\n",
        "            continue\n",
        "        X_t.append([rr[i], hr[i], spO2[i], pvcs[i]])\n",
        "        y_t.append(labels[i])\n",
        "        \n",
        "  X_t = StandardScaler().fit_transform(X_t)  \n",
        "  testing_data = np.asarray(X_t)\n",
        "  testing_count = len(testing_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "f0bI33rGGo6e"
      },
      "id": "f0bI33rGGo6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  ## This is the feed forward step in the MLP\n",
        "  ## The model is trained based on the epoch number set and within each epoch, an output value is calculated for each sample\n",
        "\n",
        "  for epoch in range(epoch_count):\n",
        "    for sample in range(training_count):\n",
        "      for node in range(H_dim):\n",
        "            preActivation_H[node] = np.dot(training_data[sample,:], weights_ItoH[:, node])\n",
        "            postActivation_H[node] = logistic(preActivation_H[node])\n",
        "      \n",
        "      preActivation_O = np.dot(postActivation_H, weights_HtoO)\n",
        "      postActivation_O = logistic(preActivation_O)\n",
        "\n",
        "      FE = postActivation_O - y[sample]\n",
        "\n",
        "      ## Backpropagation phase where the output layer feeds input back to the hidden layer and towards the input layer\n",
        "      for H_node in range(H_dim):\n",
        "        S_error = FE * logistic_deriv(preActivation_O)\n",
        "        gradient_HtoO = S_error * postActivation_H[H_node]\n",
        "                       \n",
        "        for I_node in range(I_dim):\n",
        "            input_value = training_data[sample, I_node]\n",
        "            gradient_ItoH = S_error * weights_HtoO[H_node] * logistic_deriv(preActivation_H[H_node]) * input_value\n",
        "            \n",
        "            weights_ItoH[I_node, H_node] -= LR * gradient_ItoH\n",
        "            \n",
        "        weights_HtoO[H_node] -= LR * gradient_HtoO\n"
      ],
      "metadata": {
        "id": "OmBSP3SlGtDi"
      },
      "id": "OmBSP3SlGtDi",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing phase that takes note of evaluation counts such as true positives, false positives etc \n",
        "correct_classification_count = 0\n",
        "Y_pred = []\n",
        "tp = 0\n",
        "fp = 0\n",
        "tf = 0\n",
        "ff = 0\n",
        "\n",
        "for sample in range(testing_count):\n",
        "    for node in range(H_dim):\n",
        "        preActivation_H[node] = np.dot(testing_data[sample,:], weights_ItoH[:, node])\n",
        "        postActivation_H[node] = logistic(preActivation_H[node])\n",
        "            \n",
        "    preActivation_O = np.dot(postActivation_H, weights_HtoO)\n",
        "    postActivation_O = logistic(preActivation_O)\n",
        "        \n",
        "    if postActivation_O > 0.5:\n",
        "        output = 1\n",
        "    else:\n",
        "        output = 0     \n",
        "        \n",
        "    if output == y_t[sample]:\n",
        "      correct_classification_count += 1\n",
        "      if y_t[sample]:\n",
        "        tp +=1\n",
        "      else:\n",
        "        tf +=1\n",
        "    else:\n",
        "      if y_t[sample]:\n",
        "        ff +=1\n",
        "      else:\n",
        "        fp +=1\n",
        "    Y_pred.append(output)\n",
        "    \n",
        "\n",
        "\n",
        "print('number of correct classifications', correct_classification_count)\n",
        "print('Percentage of correct classifications:')\n",
        "print(correct_classification_count*100/testing_count)"
      ],
      "metadata": {
        "id": "lEapLc54TcSI"
      },
      "id": "lEapLc54TcSI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Calculating the performance metrics to check the performance of the MLP\n",
        "\n",
        "accuracy = (tf+tp) / (tf+tp+fp+ff)\n",
        "precision = tp / (tp+fp)\n",
        "recall = tp / (tp+ff)\n",
        "f1_score = 2 * (precision*recall) / (precision+recall)\n",
        "\n",
        "print(\"accuracy:\", accuracy)\n",
        "print(\"precision:\", precision)\n",
        "print(\"recall:\", recall)\n",
        "print(\"f1_score:\", f1_score)\n",
        "\n",
        "print(tp,tf,fp,ff)\n",
        "\n"
      ],
      "metadata": {
        "id": "ukSFfGhqZ227"
      },
      "id": "ukSFfGhqZ227",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRC4vU661zmO"
      },
      "id": "ZRC4vU661zmO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}